{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch\n",
    "Elasticsearch is a distributed, oepn source search and analytics engine for all types of data, including textual, numerical, geospartial, structed and unstructed.\n",
    "\n",
    "Elasticsearch also allows you to search as you type.\n",
    "\n",
    "Elasticsearch is a NoSQL database that stores, retrieves, and amnages document-oriented and semi-structured data.\n",
    "\n",
    "Elasticsearch is build on lucene index.\n",
    "\n",
    "## Why use Elasticsearch?\n",
    "Products that involve e-ecommerce and search engines with huge databases are facing issues, including product information retrieval taking too long. This leads to poor user experinece and in turn turns off potential customers.\n",
    "\n",
    "Lag in search is attributed to the realtional database used for the product design, where the data is scattered among multiple tables -- and succussful retrieval of menaingful user information requires fetching the data from these tables. \n",
    "\n",
    "The relational databse works comparatively slowly when it comes to huge data and fetching search results through database queries.\n",
    "\n",
    "The reason why Elasticsearch is faster because it is a nosql database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Wht is ES used for?\n",
    "- Application search \n",
    "- Website search\n",
    "- Logging and log analytics\n",
    "- Geospadtial data analysis and visualization\n",
    "- Business analytics\n",
    "\n",
    "\n",
    "## Basic Concepts of Elasticsearch\n",
    "### Search Document\n",
    "A search document is the entity you put into a search engine, and it is the entity you retrieve when you run a search request.\n",
    "\n",
    "Each document has a set of fields.\n",
    "\n",
    "### Fields\n",
    "Elasticsearch works with structured JSON containing fields and values.\n",
    "\n",
    "### Cluster\n",
    "A cluster is a colleciton of one or more servers that together hold entire data and give federated indexing and search capabilities across all servers. For relational databaes, the node is DB instance. There can be N nodes with the same cluster name.\n",
    "\n",
    "### Near-Real-Time (NRT)\n",
    "Elasticsearch is a ner-real-time search platform. there is slight from the time you index a docutment untile the time it becomes searchable.\n",
    "\n",
    "### Index\n",
    "The index is a collection of documents that have similar characteristics.\n",
    "The type of index used is called an inverted index, because it inverts a page-centric data structure (page->words) ot a keyword-centric data structure (word->pages)\n",
    "\n",
    "### Node\n",
    "A node is a singer server that holds some data and participates in the cluster's indexing and querying. A node can be configured to join a spedcific cluster by the particular cluster name. A single cluster can have as many nodes as we want. A node is simple one Elasticsearch instance.\n",
    "\n",
    "### Shards\n",
    "A shard is a subset of documents of an index. An index can be divided into many shards.\n",
    "Indices is spilit horizonally into shards.\n",
    "\n",
    "### Replica\n",
    "One or More copy of index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Elasticsearch work?\n",
    "1. Raw data flows into Elasticsearch from a variety of sources, including logs, system metrics, and web applictions.\n",
    "1. Data ingestion is the process by which this raw data is parsed, normalized, and enriched before it iis indexed in Elasticsearch.\n",
    "1. Once indexed in Elasticsearch, users con run complex queries against their data and use aggregations to retrieve complex summaries of their data.\n",
    "1. From Kibana, users can create powerful visualizations of their data, share dahsboards, and manager the Elastic Stack.\n",
    "\n",
    "### Document Index Model\n",
    "\n",
    "Elastic search provide you with search at a fielded level.\n",
    "\n",
    "An entity is cut into a set of fields that comprise of individual set of values that you want to look at.\n",
    "\n",
    "When you send data to elasticsearch it is fromated as structured JSON\n",
    "\n",
    "Elasticseach creates an index for each field, the index contains every single value for that fields for every document that has came through.\n",
    "\n",
    "Elasticsearch will parse the value in a field from a string to its word components called terms, and append the index of the document to the term.\n",
    "\n",
    "<img src=\"Media/ElasticSearch-Document-Index.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "Data is stored in indexes, distributed across shards\n",
    "- Shards are primary or replica\n",
    "- Primay shard count can't be changed\n",
    "- Elasticsearch distributes shards to instans elastically\n",
    "- Primary and replica are distributed to different instances\n",
    "\n",
    "### Sharding\n",
    "You can use templates to set shard count\n",
    "\n",
    "```\n",
    "PUT <endpoint>/_template/template1\n",
    "{\n",
    "    \"index_patterns\": [\"movies*\"],\n",
    "    \"settings\": {\n",
    "        \"numbers_of_shards\": 50,\n",
    "        \"numbers_of_replicas\": 1\n",
    "    }\n",
    "}      \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Elasticsearch on MacOS\n",
    "Requires Java 11 and relavent jdk\n",
    "\n",
    "Reference https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.0-darwin-x86_64.tar.gz\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.0-darwin-x86_64.tar.gz.sha512\n",
    "shasum -a 512 -c elasticsearch-7.10.0-darwin-x86_64.tar.gz.sha512 \n",
    "tar -xzf elasticsearch-7.10.0-darwin-x86_64.tar.gz\n",
    "cd elasticsearch-7.10.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ElasticSearch/elasticsearch-7.10.0/bin\n",
    "./elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch runs on port 9200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"C02DC6F3MD6R\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"Yb0doopYRka-1WsA0mI7LA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.10.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n",
      "    \"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.7.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   538  100   538    0     0  38428      0 --:--:-- --:--:-- --:--:-- 38428\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:9200/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"C02DC6F3MD6R\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"Yb0doopYRka-1WsA0mI7LA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.10.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n",
      "    \"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.7.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   538  100   538    0     0  67250      0 --:--:-- --:--:-- --:--:-- 67250\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n",
      "1606936338 19:12:18  elasticsearch yellow          1         1      7   7    0    0        1             0                  -                 87.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   296  100   296    0     0  49333      0 --:--:-- --:--:-- --:--:-- 49333\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET \"localhost:9200/_cat/health/?v&pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Kibana on MacOS\n",
    "\n",
    "https://www.elastic.co/guide/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -O https://artifacts.elastic.co/downloads/kibana/kibana-7.10.0-darwin-x86_64.tar.gz\n",
    "curl https://artifacts.elastic.co/downloads/kibana/kibana-7.10.0-darwin-x86_64.tar.gz.sha512 | shasum -a 512 -c - \n",
    "tar -xzf kibana-7.10.0-darwin-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ElasticSearch/kibana-7.10.0-darwin-x86_64/bin\n",
    "./kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kibana runs on port 5601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://localhost:5601/app/home#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Console in Kibana for elasticsearch can be found in <br>\n",
    "http://localhost:5601/app/dev_tools#/console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"index\",\"_type\":\"_doc\",\"_id\":\"1\",\"_version\":11,\"result\":\"updated\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":11,\"_primary_term\":1}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   175  100   155  100    20   3780    487 --:--:-- --:--:-- --:--:--  4268\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -H \"Content-Type: application/json\" \\\n",
    "  -X PUT \\\n",
    "  -d '{\"name\": \"John Doe\"}' \\\n",
    "localhost:9200/index/_doc/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"index\",\"_type\":\"_doc\",\"_id\":\"1\",\"_version\":11,\"_seq_no\":11,\"_primary_term\":1,\"found\":true,\"_source\":{\"name\": \"John Doe\"}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   132  100   132    0     0  26400      0 --:--:-- --:--:-- --:--:-- 33000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET localhost:9200/index/_doc/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_index\" : \"customer\",\n",
      "  \"_type\" : \"_doc\",\n",
      "  \"_id\" : \"1\",\n",
      "  \"_version\" : 2,\n",
      "  \"result\" : \"updated\",\n",
      "  \"_shards\" : {\n",
      "    \"total\" : 2,\n",
      "    \"successful\" : 1,\n",
      "    \"failed\" : 0\n",
      "  },\n",
      "  \"_seq_no\" : 1,\n",
      "  \"_primary_term\" : 1\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   242  100   222  100    20  17076   1538 --:--:-- --:--:-- --:--:-- 18615\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X PUT \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d'{\"name\": \"John Doe\"}' \\\n",
    "localhost:9200/customer/_doc/1?pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_index\" : \"customer\",\n",
      "  \"_type\" : \"_doc\",\n",
      "  \"_id\" : \"1\",\n",
      "  \"_version\" : 2,\n",
      "  \"_seq_no\" : 1,\n",
      "  \"_primary_term\" : 1,\n",
      "  \"found\" : true,\n",
      "  \"_source\" : {\n",
      "    \"name\" : \"John Doe\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   184  100   184    0     0  36800      0 --:--:-- --:--:-- --:--:-- 36800\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET localhost:9200/customer/_doc/1?pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch REST APIs\n",
    "\n",
    "### Index (PUT/POST) API\n",
    "It helps to add or update the JSON document in an index when a request is made with the respective index with specific mapping.\n",
    "\n",
    "### GET API\n",
    "API helps to extract JSON type object by performing a get request for a particular document.\n",
    "\n",
    "### DELETE API\n",
    "You can delete a particular index, mapping or a document by sending an HTTP DELETE request to Elasticsearch.\n",
    "\n",
    "## UPDATE API\n",
    "Update the docs/data put into elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching\n",
    "Once you have digest some data into an Elasticsearch index, you can search it by sending requests to the `_search` endpoint. To access the full suite of search capabilities, you use the Elasticsearch Query DSL to specify the search criterial in the request body.. You specify the name of the index you want to search in the request URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"took\" : 0,\n",
      "  \"timed_out\" : false,\n",
      "  \"_shards\" : {\n",
      "    \"total\" : 1,\n",
      "    \"successful\" : 1,\n",
      "    \"skipped\" : 0,\n",
      "    \"failed\" : 0\n",
      "  },\n",
      "  \"hits\" : {\n",
      "    \"total\" : {\n",
      "      \"value\" : 1,\n",
      "      \"relation\" : \"eq\"\n",
      "    },\n",
      "    \"max_score\" : 0.2876821,\n",
      "    \"hits\" : [\n",
      "      {\n",
      "        \"_index\" : \"customer\",\n",
      "        \"_type\" : \"_doc\",\n",
      "        \"_id\" : \"1\",\n",
      "        \"_score\" : 0.2876821,\n",
      "        \"_source\" : {\n",
      "          \"name\" : \"John Doe\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   502  100   456  100    46  91200   9200 --:--:-- --:--:-- --:--:--   98k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d'{\n",
    "  \"query\": { \"match\": { \"name\": \"John\" } }\n",
    "}' \\\n",
    "localhost:9200/customer/_search?pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logstash\n",
    "Logstash is a plugin based data collection and processing engine.\n",
    "\n",
    "<img src=\"Media/Logstash.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "Logstash is a tool based on the filter/pipes pattern for gathering, processing and generating the logs or events. It helps to centralizing and making real time analysis of logs and envents from different sources.\n",
    "\n",
    "### General Features\n",
    "- Logstash can collect data from differfent sources and send to multiple destinations.\n",
    "- Logstash can also handle http requests and response data.\n",
    "- Logstash can handle all types of logging data like Apache Logs, Windows Event Logs, Data over Network Protocols, Data from Standard Input and many more.\n",
    "- Logstash provides a variety of filet ers, which helps the user to find more meaning in the data by parsing and transforming it.\n",
    "- Logstash can also be used for handlign sensors data in IOT.\n",
    "\n",
    "### Key Concepts\n",
    "#### Event Objects\n",
    "It is the main object in Logstash, which encapsulates the dataflow in the Logstash pipeline. Logstash uses this object to store the input data and add extra fields created during the filter stage. Logstash offers an Event API to developers to manipulate evetns.\n",
    "\n",
    "#### Pipeline\n",
    "It comprises of data flow stages in Logstash from input to output. The input data is entered in the pipeline and is processed in the form of an event. Then sends to an output destination in the user or end systerm's desired format.\n",
    "\n",
    "#### Input\n",
    "This is the first stage in the Logstash pipeline, which is used to get the data in Logstash for further processing. Logstash offers various plugins to get data from different platforms. Some of the most commonly used plugins are -- File, Syslog, Redis, and Beats.\n",
    "\n",
    "#### Filter\n",
    "This is the middle stage of Logstash, where the actual processing of events take place. A developer can use pre-defined Regex Patterns by Logstash to create sequents for differentiating getween fields in the events and criteria for accepted input events.\n",
    "\n",
    "#### Output\n",
    "This is the last stage in the Logstash pipeline, where the input events can be formatted into the structure required by the destination systems. Lastly, it sends the output evetn after complete processing to the destination by using plugins. Some of the most commonly used plugins are -- Elasticsearch, File, Graphite, Statsd, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Logstash\n",
    "- Logstash offers regex pattern sequences to identify and parse the various fields in any input envent.\n",
    "- Logstash supports a variety of web servers and data sources for extracting logging data.\n",
    "- Logstash provides multiple plugins to parse and transform the logging data into any user desirable format.\n",
    "- Logstash is centralized, which makes it easy to process and collect data from different servers.\n",
    "- Logstash uses the HTTP protocol, which enables the user to upgrade Elasticsearch versions having to upgrade Logstash in a lock step.\n",
    "\n",
    "### Disadvantages of Logstash\n",
    "- Working with Logstash can sometimes be a little complex, as it needs a good understanding and analysis fo the input logging data.\n",
    "- Filter plugins are not generic, so, the user may need to find the correct sequence of patterns to aboid error parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing logstash \n",
    "Install Logstash by running the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://artifacts.elastic.co/downloads/logstash/logstash-7.10.0-darwin-x86_64.tar.gz\n",
    "wget https://artifacts.elastic.co/downloads/logstash/logstash-7.10.0-darwin-x86_64.tar.gz.sha512\n",
    "shasum -a 512 -c logstash-7.10.0-darwin-x86_64.tar.gz.sha512\n",
    "tar -xzf logstash-7.10.0-darwin-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stashing Your first Event\n",
    "A logstash pipeline has 2 required elements, `input` and `output`, and optional element, `filter`. The input plugins consume data from a source, the filter plugin modify the data as you specify, and the output plugin write the data to a destination.\n",
    "\n",
    "<img src=\"Media/logstash-pipeline.png\" heigh=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ElasticSearch/logstash-7.10.0/bin/logstash -e 'input { stdin { } } output { stdout {} }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the command line lets you quickly test configurations without having to edit a file between iterations. The pipeline above takes input from the standard input, `stdin`, and moves the input to the sandard output, `stdout`, in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Indexing Documents\n",
    "Using put api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"index\",\"_type\":\"_doc\",\"_id\":\"2\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":9,\"_primary_term\":1}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   166  100   153  100    13  21857   1857 --:--:-- --:--:-- --:--:-- 23714\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --header \"Content-Type: application/json\" \\\n",
    "  --request PUT \\\n",
    "  --data '{\"body\":\"hi\"}' \\\n",
    "localhost:9200/index/_doc/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"index\",\"_type\":\"_doc\",\"_id\":\"2\",\"_version\":1,\"_seq_no\":9,\"_primary_term\":1,\"found\":true,\"_source\":{\"body\":\"hi\"}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   123  100   123    0     0  20500      0 --:--:-- --:--:-- --:--:-- 20500\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X GET \"localhost:9200/index/_doc/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
      "\u001b[K     |████████████████████████████████| 321 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from elasticsearch) (1.25.9)\n",
      "Requirement already satisfied: certifi in /usr/local/anaconda3/lib/python3.8/site-packages (from elasticsearch) (2020.6.20)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.10.0\n"
     ]
    }
   ],
   "source": [
    "! pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"any\": \"data\",\n",
    "    \"timestamp\": datetime.now()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_index': 'my-index',\n",
       " '_type': 'test-type',\n",
       " '_id': '42',\n",
       " '_version': 1,\n",
       " 'result': 'created',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 0,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.index(index=\"my-index\", doc_type=\"test-type\", id=42, body=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.get(index=\"my--index-index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
