{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proc directory\n",
    "/proc: directory in charge of linux performance optimization\n",
    "\n",
    "proc fidrectory has 3 parts\n",
    "1. PID directory\n",
    "2. information files /meminfo: prevents information from kernel\n",
    "3. /sys: tunables\n",
    "\n",
    "## sys\n",
    "important interface for sys\n",
    "- fs: filesystem\n",
    "- kernel: for kernel\n",
    "- net: networking\n",
    "- vm: memory\n",
    "\n",
    "swappiness: the williness of the kernel to swap\n",
    "out memory pages, if memory pressure arises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systemctl service\n",
    "```systemctl status systemd-sysct```\n",
    "its configuration is in /etc/sysctl.conf<br>\n",
    "changes to system is applied on boot<br>\n",
    "you can change swappiness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/etc/systctl.conf\n",
    "\n",
    "vm.swappiness = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful sysctl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sysctl --help\n",
    "sysctl -p /etc/sysctl.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# list sysctl prameters\n",
    "sysctl -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Kernel Module Parameters\n",
    "- Use **modinfo** to find which parameters are available\n",
    "- Use **modprobe module key=value** to specify a parameter in runtime\n",
    "- Use /etc/modprobe.d/modulename.conf to specify a permanent parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting Resource Usage\n",
    "\n",
    "### ulimit\n",
    "ulimit is the old way of configuring resource usage<br>\n",
    "Applying POSIX Resource Limits\n",
    "- Set runtime limits with **ulimit**\n",
    "- - It dapplies to the shell in which the command is used\n",
    "- Apply persistent ulimit settings to /etc/security/limits.conf\n",
    "- Soft limits can be modified by the user, hard limits implements an absolute ceiling\n",
    "- - Users can set soft limits, but only lower than the enforced hard limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /etc/security/limits.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Persistent limits\n",
    "- **pam_limits** is applied from PAM sessions to configure persistent limits\n",
    "- It works with /etc/security/limits.conf and /etc/security/limits.d/*.conf\n",
    "- See *man 5 limits.conf** for documentation\n",
    "- - Notice that not all llimites currently work on RHEL. The **rss**limit for instance doesn not work\n",
    "- Example: set the maximum amount of logins for studentst to 2, creating a file with the contents /etc/security/limits/students.d and the following contents:\n",
    "- - **@students hard maxlogins 2**\n",
    "\n",
    "#### Setting Limits to Services\n",
    "- In systemd units, add the **LIMIT\\** entries to the **[Service]** block of a unit file to limit what services can do\n",
    "- If, for inatance you want to allow your blah.service a maximum of 60 seconds of CPU time before it is killed, add the following to **/etc/systemd/system/blah.service.d/10-cpulimits.conf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Service]\n",
    "LimitCPU=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **systemctl daemon-reload** and **systemctl restart blah** to make the changes effective\n",
    "- See **man5 systemd.exec** and **man 2 setrlimit** for a full list of Limit\\* settings\n",
    "\n",
    "PAM (Pluggable Authentication Modules): it shapes what is happening when user log into the system\n",
    "\n",
    "<img src='screenshots/PAM.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "man systemd.exec | grep limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /etc/systemd/system/sleep.service.d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/sleep.service.d/10-cpulimits.conf\n",
    "[Service]\n",
    "LimitCPU=60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# to reload\n",
    "systemctl daemon-reload\n",
    "systemctl restart\n",
    "status sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Groups\n",
    "Understand Control Groups\n",
    "- Control Groups place resources in controllers that represent the type of resource\n",
    "- Some common default controllers are **cpu**, **memory**, and **blkio**\n",
    "- These controllers are subdivided in to a tree structure where different weights or limits are applied to each branch\n",
    "- - Each of these branches is a cgroup\n",
    "- - One or more processes are assigned to a cgroup\n",
    "- Cgroups can be applied from the command line, or from systemd\n",
    "- - Manual creation happened throught the **cgconfig** service and the **cgred** process\n",
    "- - In all cases, cgroup settings are written to /sys/fs/cgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /sys/fs/cgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Allocation \n",
    "- Machine\n",
    "- System\n",
    "- User\n",
    "\n",
    "<img src='screenshots/CGroup.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Cgroups in Systemd\n",
    "Systemd divides **cpu**, **cpuacct**, **blkio** into slices\n",
    "- **system** for system processes and daemons\n",
    "- **machine** for virtual machines\n",
    "- **user** for user sessions\n",
    "On a systemd-system, system-enabled cgroups can be omitted and you can still use **cgconfig** and **cgred**. See **systemd-system.conf** for instructions on how to do that and make sure to rebuild initramfs to make this effective\n",
    "\n",
    "#### Using Custom Slices\n",
    "- Administrators can create their own slices, naming them *.slice, or slices within a slice, using the **parent-child.slice** naming\n",
    "- Child slices will inherit the settings of the parent slices\n",
    "- Make sure to turn on CPU, memory, or I/O accounting to see how they are used within a slice\n",
    "\n",
    "#### Enabling Accounting\n",
    "Enabling accounting in the [Service] section of the unit file\n",
    "- **CPUAccounting=true**\n",
    "- **MemoryAccounting=true**\n",
    "- BlockIOAccounting=true**\n",
    "Or better: enable it in /etc/systemd/system.conf\n",
    "Use drop-in files to take care of this\n",
    "- e.g. the SSH service would use a drop-in /etc/systemd/system/sshd.service.d/*conf<br>\n",
    "**man 5 systemd.resource-control** for all parameters\n",
    "- CPUShares=512\n",
    "- MemoryLimit=512M\n",
    "- BlockIO*="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# listing folder for cpu\n",
    "ls /sys/devices/system/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```top``` then 1 to check cpu usage\n",
    "\n",
    "### Managing slice\n",
    "Putting Commands into a Slice<br>\n",
    "To put a command into a slice, you can use the **systemd-run** command with the **slice=** option\n",
    "- **systemd-run --slice=example.slice sleep 10d**\n",
    "- show with **systemd-cgls /example.slice/<servicename>**\n",
    "- If the **--slice** option is not used, commands started with **systemd-run** will be put into the **system.slice**  \n",
    "    \n",
    "Using Custom Slices\n",
    "- Put a service into a custom slice using **Slice=my.slice**; if the slice doesn't exist it iwll be created when the service is started\n",
    "- You can also pre-create custom slices by creating a *.slice file in /etc/systemd/system. Put the tunables in the **[Slice]** section\n",
    "- To make a lice a child of another slice, git it a name **<parent>-<clild>.slice**; it will inherit all settings of the parent slice\n",
    "- Note that the slice will only be created once the first process is started within\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! systemd-cgtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! systemd-cgls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "- Benchmarking is comparing performance characteristics to industry standards\n",
    "- This means that data needs to be gathered and compared\n",
    "- In IT there are often is no such thing as a standard benchmark, so you'll need to gather and compare a lot \n",
    "- Benchmarking is NOT profiling, which is about gathering information about performance hot spots\n",
    "\n",
    "## Subsystem involved\n",
    "While benchmarking, different subsystems should be involved\n",
    "- Processor\n",
    "- Memory\n",
    "- Scheduler\n",
    "- I/O\n",
    "- Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Utilities\n",
    "- **vmstat**\n",
    "- - Notive that the first line of vmstat output is giving the average since boot!\n",
    "- **iostat**\n",
    "- **mpstat**\n",
    "- **sar**: can be used for gathering performance data\n",
    "- **awk**: is useful for data analysis\n",
    "- **gnuplot**: can be useful for plotting data\n",
    "- **pcp**: is useful as an extensive testing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Performance data with sar\n",
    "**sar**\n",
    "- For filtering purposes, use **LANG=C sar -b**\n",
    "- Create an alias to do this automatically\n",
    "- Consider /etc/sysconfig/sysstat for additional settings\n",
    "- Data collection interval is set through cron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gnuplot\n",
    "you can plot performance data with Gnuplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Information\n",
    "with dmesg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dmsg | less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU, OS and cache Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "lscpu\n",
    "x86info -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Architecture\n",
    "- Cache is organized in lines, and each line can be used to cache a specific location in memory\n",
    "- Each CPU has its separate cache and its own cache controller\n",
    "- If a processor references main memory, it first checks cache for data. If it's there, then that is referred to as a cache hit\n",
    "- A cache line fill occurs after a cache miss, and means that data is loaded from main memory\n",
    "### Write-through and Write-back\n",
    "- If write-through caching is enabled, when a line of cache memory is updated, the line is updated in memory as well\n",
    "- If write-back is enabled, the write to cache is only written to main memory at the moment the cache line is deallocated\n",
    "- Write-back is more efficient, write-through ensures a higher state of stability\n",
    "- If on multi-CPU systems changes are not committed to memory immediately, the other CPUs need to be updated that something is changed if they are caching it also\n",
    "- - This is referred to as cache snooping, which is a hardware feature\n",
    "### Other Cache Features\n",
    "- Direct mapped cache means that each line of cache can only cache a specific location in main memory\n",
    "- - This is the cheaper solution\n",
    "- Fully associative cache means that a cache line can cache any location in main RAM\n",
    "- - More flexible, more expensive\n",
    "- Set associative cache offers a compromise between direct mapped and fully associated cache, nd allows a memory location to be cache into n lines of cache, where n can be a number like 2 for instance\n",
    "### Locality of Reference\n",
    "- Cache memory is most efficient when majority of memory access come from cache\n",
    "- Programs that access memory sequentially benefit most from cache\n",
    "- Somethimes within a program, routines arfe used to make memory access less efficient regarding the way cache is accessed. If that is the case, different **gcc** compiler options can be usede for optimization of cache access\n",
    "- - These options are -O, -O2, -O3, and -Os, and can be passed while compiling using the -f option to gcc\n",
    "- - The -O option are optimizing gcc output for size in different ways. Consult the man page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "System and Library Calls\n",
    "- The kernel exposes system calls to provide kernel access to applications\n",
    "- - While executing system calls, the application claims kernel time, which is known as system time (and visible in top such as)\n",
    "- A library call is the application way of providing functions\n",
    "- - Tiume spent handling library calls is seen as user time in \n",
    "\n",
    "## Using **strace**\n",
    "- **strace**: is used to trace system calls\n",
    "- **strace \\< command \\>**: will show what the command is doing\n",
    "- **strace -p \\< PID \\>**: gives information about a PID\n",
    "- **strace -c \\< command \\>**: shows counters and thus insight on who is doing what\n",
    "- **strace -f**: follows childl processes as sell, which by default is not the case\n",
    "- **strace -e** allows you to follow specific system calls only, as in **strace -e open ls**\n",
    "\n",
    "**ltrace** is similar to strace\n",
    "\n",
    "you can use strace for program that never finishes for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% time     seconds  usecs/call     calls    errors syscall\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      " 22.71    0.000094           9        11           close\n",
      " 21.50    0.000089          45         2           getdents\n",
      " 16.67    0.000069           8         9           openat\n",
      " 12.08    0.000050           5        10           fstat\n",
      "  8.45    0.000035          35         1           write\n",
      "  6.76    0.000028          14         2         2 ioctl\n",
      "  6.28    0.000026           2        17           mmap\n",
      "  5.56    0.000023           3         8         8 access\n",
      "  0.00    0.000000           0         7           read\n",
      "  0.00    0.000000           0        12           mprotect\n",
      "  0.00    0.000000           0         1           munmap\n",
      "  0.00    0.000000           0         3           brk\n",
      "  0.00    0.000000           0         2           rt_sigaction\n",
      "  0.00    0.000000           0         1           rt_sigprocmask\n",
      "  0.00    0.000000           0         1           execve\n",
      "  0.00    0.000000           0         2         2 statfs\n",
      "  0.00    0.000000           0         1           arch_prctl\n",
      "  0.00    0.000000           0         1           set_tid_address\n",
      "  0.00    0.000000           0         1           set_robust_list\n",
      "  0.00    0.000000           0         1           prlimit64\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      "100.00    0.000414                    93        12 total\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -c ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+++ exited with 0 +++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -e open ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/selinux/config\", F_OK)     = -1 ENOENT (No such file or directory)\n",
      "+++ exited with 0 +++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -e access ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel tracing with **ftrace**\n",
    "ftrace was originally developed for function training in the linux kernel<br>\n",
    "Also gives access to events using static kernel traces\n",
    "- System calls\n",
    "- Scheduler events\n",
    "- Memory management\n",
    "- Interrupts\n",
    "It uses static traces that are present in the kernel by default<br>\n",
    "Using ftrace\n",
    "- Plugin provide new trace types\n",
    "- Exposed via **debugfs** in /sys/kernel/debug/tracing\n",
    "- User-space **trace-cmd** tools are used to access traces\n",
    "- - Results are written yo a file trace.dat\n",
    "- Notice that tracing does cause (a log) of overhead!\n",
    "- - Use filters: **trace-cmd record -e sched_switch -f ' prev)prio < 100'**\n",
    "- - Exclude specific: **trace-cmd record -e sched -v -e \"\\*static\\*'**\n",
    "\n",
    "Using **trace-cmd**\n",
    "- **trace-cmd** uses events and plugins\n",
    "- man pages are available\n",
    "- - **trace-cmd list**: shows available plugins\n",
    "- First, start capturing traces\n",
    "- - **trace-cmd record** dumps all trace data\n",
    "- - **trace0cmd record -p function_traph touch /tmp/file** traces\n",
    "- Use **trace-cmd report** to see the result\n",
    "- - User filtering for more specific results in **trace-cmd report | grep selinux**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "yum install trace-cmd\n",
    "trace-cmd record -p function_graph touch /tmp/file\n",
    "trace-cmd report\n",
    "trace-cmd report | grep selinux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SystemTap\n",
    "SystemTap is for monitory your linux system while running applications for more information read<br>\n",
    "https://sourceware.org/systemtap/SystemTap_Beginners_Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Workflow\n",
    "<img src='screenshots/IO_Workflow.png' style='height: 50%; width: 50%'>\n",
    "## I/O to storage\n",
    "Working with Block Devices\n",
    "- Data is written in blocks\n",
    "- Block devices are accessible through device nodes in /dev\n",
    "- These support seeking for specifc locations\n",
    "- Page cache is sued to optimize performance\n",
    "- - Bypass page cache by using unbuffered I/O\n",
    "- Smart block devices don't need OS level optimization\n",
    "- - They have large caches\n",
    "- - User DMA for memory access\n",
    "- - And do their own I/O scheduling\n",
    "- - With these, you're better off passing I/O directly to the device\n",
    "Understanding I/O Challenges\n",
    "- HDDs have a delay because the read/write head needs to move to the right position\n",
    "- - Seek time is where the hard drive positions the head over the right track\n",
    "- - Rotational delay is where the HDD waits for the right sector to pass under the heads\n",
    "- If data is spread out over the disk, a lot of time is lost\n",
    "- - Disk controller movements can be minimized by re-arranging disk requests\n",
    "- - RHEL does this automatically, putting the requests in a queue and running an elevator algorithm\n",
    "- - In elevator algorithms, starvation can occur in bad algorithms: only floors in the middle are getting serviced\n",
    "\n",
    "I/O Schedulers\n",
    "- **noop**: FIFO: First requests that come in the handled first. Used for SAN, hypervisors and SSD\n",
    "- **deadline**: queued requests for executed in batches defined in the **fifo_batch** parameter. Deadline is good for file and database servers\n",
    "- - Higher values = enhanced throughput\n",
    "- - Lower values = low latencies\n",
    "- - **Read/write expire** define maximum waiting time\n",
    "- - Read requests get priority; give read request more priority by increasing the value in **writes_starved**\n",
    "- **cfq**(complete fair queueing) is useful if many processes are operating on the disk at the same time. Use **ionice** to the cgroup **blkio** controllers to set priorities in this scheduler\n",
    "- - Do NOT user on servers\n",
    "- - Use **ionice -c n -p PID** where n is between 0-7 and 0 is highest priority\n",
    "\n",
    "Selecting I/O Schedulers\n",
    "- As a boot time kernel argument\n",
    "- - elevator=\n",
    "- Throught/sys/block/sda/queue/scheduler\n",
    "- - Each schedudler has its own tuables in /sys/block/sda/queue/iosched\n",
    "- - Change by echoing a new value in the scheduler file\n",
    "- Using tuned profiles\n",
    "- - In [disk] section, use elevator="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management\n",
    "- Memory is prgainized in pages, 4 KiB by default\n",
    "- Processes have a major and minor page fault counter\n",
    "- - *ps* will show you\n",
    "- Systemd can be used to enforce memory limits\n",
    "- Linux uses virtual memory that is mapped to physical memory\n",
    "- - Use **pmap** to display\n",
    "\n",
    "tlb maps virtual memory to resident memory\n",
    "\n",
    "<img src='screenshots/virtual-resident-memory.png' style='height: 50%; width: 50%'>\n",
    "\n",
    "Understanding Memory and Paging\n",
    "- Physical RAM is divided into page frames and the OS uses memory pages tp address memory\n",
    "- - A page size normally is 4KiB\n",
    "- Processes have a virtual address space. From there, physical memory pages are mapped\n",
    "- - Processes can only see their physical memory pages\n",
    "- - On 32 bit systems, the virtual address space is maxed to 4 GiB, on 64 bits address space is 16 EiB\n",
    "- - Notice that the 16 EiB is a theoretical limit\n",
    "- Virtual vs physical memory mappings are monitored using **top** or **ps**\n",
    "- Physical memory pages can be shared between processes. If this is the case, they count to the resident memory of each process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the TLB (Translation Lookaside Buffer)\n",
    "- Each process needs its own page table that contains mapping of virtual addresses to physical addresses\n",
    "- Each virtual page has an entry in this table. If this entry is not mapped to an physical page, this is a page fault\n",
    "- - A Major page fault occurs if a page is swapped out or needs to be loaded from a file on disk\n",
    "- - A minor page fault occurs if the process has just loaded and the virtual page still needs to be mapped to a physical page\n",
    "- - Monitor through /proc/\\< PID \\> /stat\n",
    "- If each process has a complete table, it would require lots of memory\n",
    "- To mitigate that problem, the page table is organized hierarchically, and only page tables that contain physical addresses are administered\n",
    "- Looking up a page is expensive\n",
    "- For that reason, the Translation Look aside Buffer (TLB) is used\n",
    "- - TLB is a hardware cache on CPU\n",
    "- - Depending on hardware, the TLB cache can be organized in L1 and L2, differentiating between instructions and data\n",
    "- - It caches page mappings the process has recently used\n",
    "- Use **x86info -c** to find the size of the TLB\n",
    "- - The TLB is relative small\n",
    "- - To optimize its use, huge pages can be used\n",
    "\n",
    "Memory CoW\n",
    "- New processes are created by forking the parent processes\n",
    "- - A duplicate process is created\n",
    "- - Pages are marked as copy-on-write, which means that data is modified while duplication occurs\n",
    "- Where data can be shared between parent and child, the child has pointers to read-only parts of parent memory address space\n",
    "- The data is only really copied to the child when written to, which leads to performance improvement\n",
    "\n",
    "Managing Proces Memory\n",
    "- Use **ps o pid, comm, minflt, majflt** to get information about page faults that have occurred\n",
    "- - Minor page faults cannot be avoided, major page faults should be avoided\n",
    "- To limit the amount of available memory to process, use Systemd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cgroups\n",
    "[Service]\n",
    "MemoryLimit=4G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- monitor current virtual address space usage using **pmap \\< pid \\>** or **cat/proc/ \\< PID \\> /maps** and **/proc/ \\< PID \\> /smaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding **pmap** output\n",
    "- **pmap** shows virtual address use - no information about RSS\n",
    "- Notice that not all virutal memory is mapped\n",
    "- - The mapping always startts at 0x00400000 (4 MiB) for the executable part of the process that comes from the executable file\n",
    "- - Next, there is the heap which is memory that has been dynamically allocated using **malloc** and shared libraries\n",
    "- - From address 0x07fffffffffff (128 TiB) on, there is the stack which contains anonymous memory that is requested by the process\n",
    "- - The kernel is available from address -xffff8000000\n",
    "- If a process tries to access memory that doesn't occure in the virtual memory map, the kernel gives a SIGSEGV - a segmentation faul\n",
    "- - If that happens, the program stops and starts core dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pmap `ps | head -2 | cut -d\" \" -f1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uderstanding Memory Leaks\n",
    "- After using it, processes should return memory they have been using, If this doesn't happen, then this is known as a memory leak\n",
    "- Memory leaks are irrelevant for short-time prunning processes such as **ls**, as all memory will be freed by the kernel when it stops running\n",
    "- For daemon processes, meory leaks are a severe problem\n",
    "- The only fix for memory leak is to kill and restart the process\n",
    "\n",
    "### Types of Memory Leak\n",
    "2 types virtual and resident\n",
    "- If a program requests memory but doesn't use it, the virtual size of program memory goes up but no physical memory is used - this is a leark in virtual memory\n",
    "- - The total amount of virtual memory that is allocated is visiable as **Committed_AS** in **/proc/meminfo**\n",
    "- If the program starts mapping resident memory, a problem occurs and you'll suffer from it\n",
    "\n",
    "### Finding Memory leaks\n",
    "- The best tool to find memory leak is **valgrind**. Use **valgrind --tool=memcheck program** to do so\n",
    "- Use **--leak-check=full** as an option to get information about which funciton is leaking memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "valgrind --tool=memcheck ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Reclamation\n",
    "A memory page can be in differetn states\n",
    "- Free: immediately available\n",
    "- Inactive clean: it is not used and its contents is synchronized with corresponding data on disk. It can be treated as a free page, but\n",
    "- - this will result in a page fault in the process accesses it again\n",
    "- - the page needs to be swapped if it is anonymous memory\n",
    "- Inactive dirty: it isn't used but the page contents has not been synchronized to disk\n",
    "- Active: it's doing something\n",
    "Monitoring Memory States\n",
    "- /proc/meminfo gives a generic overview\n",
    "- /proc/PID/smaps shows sizes of Shared/Private clean and dirty memory\n",
    "- Dirty pages need to be written to disk\n",
    "- - Recent Red Hat uses a per-backing device flush thread to flush dirty data, it shows as flush-MAJOR:MINOR\n",
    "\n",
    "MAJOR:MINOR is showned using lsblk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lsblk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing OOM\n",
    "- OOM is out of memory\n",
    "- As most applications never use their entire address space, Linux uses memory overcommitting\n",
    "- As a result, you may get in an OOM situation\n",
    "- Set 3 different modes for overcommiting through **vm.overcommit_memory**\n",
    "- - 0: Heuristic overcommit (default). Overcommitting is allowed, unless it's a very large unrealistic request\n",
    "- - 1: Always overcommit\n",
    "- - 2: Ratio based overcommit: based on available RAM + swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding OOM\n",
    "- It occurs when a minor page fault happens but no free pages are available\n",
    "- If OOM happens, the OOM killer becomes active and will kill one or more processes to free memory\n",
    "- - You can trigger this using **echo f > /proc/sysrq-trigger** and read output in **dmesg**\n",
    "- - Find which trigger by using **echo h > /proc/sysrq-trigger**\n",
    "- This is bad, very bad, which is why alternatively you can set **vm.panic_on_oom = 1** to have the kernel panic instead\n",
    "- Better avoid getting into OOM at all times!\n",
    "\n",
    "## OOM Killer\n",
    "- Every process has an **oom_score** in **/proc/PID/oom_score**\n",
    "- Higher scores are more likely to get killed\n",
    "- The kernel and systemd are immune, root processes, processes with a higher runtime and processes involved in direct hardware ac*. It's value will be added to **oom_score**\n",
    "\n",
    "## Memory Zones\n",
    "- You may get in an OOM situation, evnen if **free** stills reports available memory (in particular on 32-bit systems)\n",
    "- This is because the Linux kernel works in memory zones\n",
    "- - Zone DMA goes from 0 MiB to 16 MiB\n",
    "- - Zone DMA32 goes from 16 MB to 4 GB\n",
    "- - Zone Normal goes from 4 GB to end of available memory\n",
    "- The above is for 64-bit systems. 32-bit systems have low memory up to 896 MiB, all above is high memory\n",
    "- Monitor /proc/boddyinfo to get information aobut these different zones\n",
    "\n",
    "\n",
    "DMA stands for Direct Memory Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://learning.oreilly.com/videos/linux-performance-optimization/9780134985961/9780134985961-LPOC_03_09_07sysctl -a | grep overcom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU\n",
    "CPUs have different components that are important for performance optimization\n",
    "- Socket: slot contains one or more mechanical components providing mechanical and electrical connections between a microprocessor and a printed circuit board. Is the connector on the motherboard that houses a CPU and forms the electrical interface and contact with the CPU\n",
    "- Core\n",
    "- Registers\n",
    "- Cache\n",
    "- Memory Controller\n",
    "- External bus\n",
    "Other attribute also play a role\n",
    "- Architecture\n",
    "- CPU family\n",
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "lscpu # display information about cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CPU-bound Tasks\n",
    "- A task is CPU-bound when CPU availability is the limiting factor\n",
    "- Different factors can influence CPU-speed\n",
    "- - The number of other tasks that also need attention from the CPU\n",
    "- - Efficiency of hardware cache\n",
    "- - Type of instruction that is executed\n",
    "- Executing an instruction takes different steps\n",
    "- - The fetch unit looks up the instruction in the L1i dand L2i hardware caches, or fetches it from RAM and moves to cache\n",
    "- - The instuction is processed by the instruction decoder\n",
    "- - Instruction-related data is fetched\n",
    "- - Instruction is sent to the execution unit \n",
    "- - And it will run\n",
    "\n",
    "## Process Scheduler\n",
    "<img src='screenshots/Process-Scheduler.png' style='height: 50%; width: 50%'>\n",
    "- The kernel Process Scheduler determines which process to run when\n",
    "- It need to meet different criteria\n",
    "- - Quickly determine which process to run next\n",
    "- - Give each process a fair share of time\n",
    "- - Distinguish between high priority and low priority processes\n",
    "- - Be responsive to application requests\n",
    "- - Be predictabloe and scalable\n",
    "- Because all of these are hard to unite in 1 solution, different CPU schduler solutions do exists\n",
    "\n",
    "### Understanding the O(1) Scheduler\n",
    "- The legacy O(1) Scheduler works with 2 queues: a run queue and an expired queue\n",
    "- The processor services the process with the highest priority in the run queue\n",
    "- When the time slice of a process expires, the scheduler moves the process to the appropriate position in the expired queue (accordiing to priority)\n",
    "- It next runs the highest priority process from the queue and repeats the process\n",
    "- When the run queue has completely been serviced, it makes the expired queue, the active queue and the process starts again\n",
    "\n",
    "### Understanding Complete Fair Scheduling\n",
    "- CFS uses a red-black tree that is based on virtual time\n",
    "- Virtual time is based on the time waiting to run, the number of processes that need CPU time, and process priority\n",
    "- The process with the most virutal time (which should be the process that has been waiting longest) gets CPU time\n",
    "- By using CPU time the virtual time decreases\n",
    "- Once it no longer has the most virtual tie, it gets pre-empted\n",
    "- Using this approach makes it easier to preven users that are claiming too much time to seriously hurt performance\n",
    "\n",
    "### CFS **sysctl** Parameters\n",
    "- **sched_latency_ns**: epoch duration in nanoseconds\n",
    "- **sched_min_granularity_ns**: granularity of epoch in ns. If the number of running tasks in the queue is greater **sched_latench_ns** divided by **sched_min_granularity_ns** there is too many tasks on the system and epoch length must be increased\n",
    "- **sched_migration_cost_ns**: if the real running time of a process is longer than this value, the scheduler will try not to move it to another CPU\n",
    "- **sched_rt_period__us**: the time slice that is defined to run RT processes\n",
    "- **sched_rt_runtime_us**: maximum CPU time that can be used by all real time tasks in a **sched_rt_period_us** time period\n",
    "\n",
    "## Managing CPU Scheduling\n",
    "- Systemd CGroups CPUShares can be used to set run-time priority\n",
    "- Do this run-time using **systemctl set-property vsftpd.service -- runtime CPUShares=512**\n",
    "- Skip **--runtime** to make it persistent\n",
    "- Or create a conf file in /etc/systemd/system/\\< srvice\\> .service.d/ that has CPUShares=512 in the [Service] section\n",
    "    \n",
    "## Managing Real-Time Scheduling\n",
    "- **SCHED_RR**: Round Robin real-time scheduler which has a process only run until its time slice is expired\n",
    "- **SCHED_FIFO**: runs until blocked by I/O calls, **sched_field** or a higher priority process comes along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undrstand Process Priority Numbers for Kernel\n",
    "- Kernel level system priority goes from 0 to 99, where a highter number is the lower priority (0 is the highest priority)\n",
    "- Kernel level real time priority ges from 00 to 0, where a lower number is the lower priority (99 is hte highest priority)\n",
    "- Real time priority 99 corresponds to system priority 0\n",
    "- The **nice** command affects non-real time processes only, which run with a system priority of 99\n",
    "- The **top** command just shows RT fro real time processes, 20 for \"regular\" processes, which can be adjusted by using the nice command\n",
    "- No matter what you do with the **nice** command, it doesn't change the real kernel priority of the process\n",
    "\n",
    "### Managing Real-time\n",
    "For a sysadmin, it is important to ensure that the system stays responsive. To do so, use\n",
    "- **kernel.sched_rt_period_us** to define the CPU allocation time frame\n",
    "- **kernel.sched_rt_runtime_us** to define the share of time frame that can be allocated for real time\n",
    "- - By default this is set to 0.95 seconds, which always leaves 0.05 seconds per second for processes in the **SCHED_OTHER** queue\n",
    "- - If set to 0, no time periods can be allocated to real-time processes\n",
    "\n",
    "### Non-real Time Scheduling\n",
    "- **SCHED_IDLE** is for running very low priority applications\n",
    "- - This ruins processes with a priority that is lower than **nice 19**\n",
    "\n",
    "## Change Process Schedulers\n",
    "- An application can use the syscall *sched_setscheduler** to set the scheduler\n",
    "- Or administrators can use **chrt** to do so: **chrt [scheduler] priority command**\n",
    "- - **-b** runs SCHED_BATCH\n",
    "- - **-f** runs SCHED_FIFO\n",
    "- - **-i** runs in SCHED_IDLE\n",
    "- - **-o** runs on SCHED_OTHER\n",
    "- - **-r** runs in SCHED_RR\n",
    "- Use for instance **chrt -p -f 10 \\$(cat /var/run/vsftpd.pid)** to run the vsftpd process in SCHED_FIFO\n",
    "- Documentation in /usr/share/doc/kernel-doc-*/Documentation/scheduler after install ```yum install kernel-doc```\n",
    "\n",
    "To see scheduler option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show or change the real-time scheduling attributes of a process.\n",
      "\n",
      "Set policy:\n",
      " chrt [options] <priority> <command> [<arg>...]\n",
      " chrt [options] --pid <priority> <pid>\n",
      "\n",
      "Get policy:\n",
      " chrt [options] -p <pid>\n",
      "\n",
      "Policy options:\n",
      " -b, --batch          set policy to SCHED_BATCH\n",
      " -d, --deadline       set policy to SCHED_DEADLINE\n",
      " -f, --fifo           set policy to SCHED_FIFO\n",
      " -i, --idle           set policy to SCHED_IDLE\n",
      " -o, --other          set policy to SCHED_OTHER\n",
      " -r, --rr             set policy to SCHED_RR (default)\n",
      "\n",
      "Scheduling options:\n",
      " -R, --reset-on-fork       set SCHED_RESET_ON_FORK for FIFO or RR\n",
      " -T, --sched-runtime <ns>  runtime parameter for DEADLINE\n",
      " -P, --sched-period <ns>   period parameter for DEADLINE\n",
      " -D, --sched-deadline <ns> deadline parameter for DEADLINE\n",
      "\n",
      "Other options:\n",
      " -a, --all-tasks      operate on all the tasks (threads) for a given pid\n",
      " -m, --max            show min and max valid priorities\n",
      " -p, --pid            operate on existing given pid\n",
      " -v, --verbose        display status information\n",
      "\n",
      " -h, --help           display this help\n",
      " -V, --version        display version\n",
      "\n",
      "For more details see chrt(1).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chrt --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCHED_OTHER min/max priority\t: 0/0\n",
      "SCHED_FIFO min/max priority\t: 1/99\n",
      "SCHED_RR min/max priority\t: 1/99\n",
      "SCHED_BATCH min/max priority\t: 0/0\n",
      "SCHED_IDLE min/max priority\t: 0/0\n",
      "SCHED_DEADLINE min/max priority\t: 0/0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chrt -b --max ## shows priority available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid 10's current scheduling policy: SCHED_FIFO\n",
      "pid 10's current scheduling priority: 99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chrt -p -f 10 $(pidof ssh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Pinning\n",
    "Managing CPU Pinning\n",
    "- Everytime the scheduler reschedules a process, it will determine which CPU it will go on\n",
    "- This is bad for cache usage\n",
    "- Use the systemd CPUAffinity setting in the [Service] block to pin a service to a CPU\n",
    "- - CPUAffinity=0, 1 allows the process to run on CPU cores 0 and 1 only\n",
    "- - If you use this setting on a NUMA system, make sure to start the **numad.service** as well to manage NUMA usage\n",
    "- For more scalability, use the cpuset Cgroup to manage multiple feature (not currently supported by systemd)\n",
    "\n",
    "Understanding CPUSet\n",
    "- The cpuset Cgroup offers some useful parameters\n",
    "- - **cpuset.cpus**: the CPUs that can be inside this group\n",
    "- - **cpuset.mems**: the NUMA memory zpnes in this group. On non-NUMA systems, always use this parameter and set to 0\n",
    "- - **cpuset.{cpu, memory}_exclusive**: set to 1 if the CPU or mmemory for this group is exclusive, making it not accessible for other processes\n",
    "- As systemd do not currently supports cpuset, define an ExecStartPost script within systemd unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cpuset ExectStartPost script Example\n",
    "#! /bin/bash\n",
    "mkdir -p /sys/fs/cgroup/cpuset/cpuset0\n",
    "echo 0 > /sys/fs/cgroup/cpuset/cpuset0/cpuset.cpus\n",
    "echo 0 > /sys/fs/cgroup/cpuset/cpuset.mems\n",
    "for PID in $(pgrep httpd); do\n",
    "    echo %{PID} > /sys/fs/cgrooup/cpuset/cpuset0/tasks\n",
    "done\n",
    "\n",
    "[service]\n",
    "ExectStartPost=/usr/local/bin/cpuset0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUAffinity=\n"
     ]
    }
   ],
   "source": [
    "! systemctl show --all sshd.service | grep CPUAffinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash \n",
    "cp /user/lib/service/system/sshd.service .\n",
    "vi sshd.service\n",
    "## then insert CPUAffinity='number'\n",
    "\n",
    "systemclt daemon-reload\n",
    "systemctl restart sshd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "! systemctl sho --all ssh.service | grep CPUAffinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an exec start script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /usr/local/bin\n",
    "cd cpu.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu.sh\n",
    "#! /bin/bash\n",
    "mkdir -p /sys/fs/cgroup/cpuset/cpuset0\n",
    "echo 0 > /sys/fs/cgroup/cpuset/cpuset0/cpuset.cpus\n",
    "echo 0 > /sys/fs/cgroup/cpuset/cpuset0/cpuset.mems\n",
    "for PID in $(pgrep sshd); do\n",
    "    echo ${PID} > /sys/fs/cgroup/cpuset/cpuset0/tasks\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Interrupts\n",
    "- Interrupts are used to indicate that there is wrok to do at this moment\n",
    "- When it occurs, a CPU needs to be chosen\n",
    "- Monitor /proc/interrupts for an overview of which is handled when\n",
    "- User **/proc/irq/ \\< number \\> /smp_affinity** to determine which interrupt is handled where\n",
    "\n",
    "The **smp_affinity** value is set as a bitmask which is represented as a hexadecimal number\n",
    "\n",
    "| bitmask | CPU | HEX|\n",
    "| -- | -- | -- |\n",
    "| 00000001 | 0 | 0x1 |\n",
    "| 00000010 | 1 | 0x2 |\n",
    "| 00000100 | 2 | 0x4 |\n",
    "| 00001000 | 3 | 0x8 |\n",
    "| 00010000 | 4 | 0x10 |\n",
    "| 00100000 | 5 | 0x20 |\n",
    "| 01000000 | 6 | 0x40 |\n",
    "\n",
    "- CPU affinity can beset to multiple CPUs\n",
    "- For instance, 01010010 would set affinity to CPUs 2,5, and 7\n",
    "- This is equal to 0x92\n",
    "- Calculate from the bash shell as n^2 using 2**n\n",
    "- - printf '%0x' \\$[2\\*\\*2+2\\*\\*5+2\\*\\*7] > /proc/irq/ \\< n \\> /smp_affinity\n",
    "\n",
    "\n",
    "\n",
    "Using **irqbalance**\n",
    "- The **irqbalance** service adjusts the smp_affinity of all interrupts every 10 seconds\n",
    "- This increases the chance of getting cache hits\n",
    "- No effect on single-core or dual-core systems with shared cache\n",
    "- Settings in /etc/sysconfig/irqbalance\n",
    "- - IRQBALANCE_ONESHOT: set to yes to run once\n",
    "- - IRQBALANCE_BANNED_CPUS: set as a hexadecimal bitmask to exclude CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /proc/interrupts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /proc/irq/3 # some folder number that exists\n",
    "printf '%0x' $[2**2+2**5+2**7] > /proc/irq/3/smp_affinity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● irqbalance.service - irqbalance daemon\n",
      "   Loaded: loaded (/lib/systemd/system/irqbalance.service; enabled; vendor preset: enabled)\n",
      "   Active: active (running) since Wed 2019-09-04 01:39:53 PDT; 1 day 3h ago\n",
      " Main PID: 1119 (irqbalance)\n",
      "    Tasks: 2 (limit: 4915)\n",
      "   CGroup: /system.slice/irqbalance.service\n",
      "           └─1119 /usr/sbin/irqbalance --foreground\n",
      "\n",
      "Sep 04 01:39:53 yi-XPS-13-9380 systemd[1]: Started irqbalance daemon.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "systemctl status irqbalance.service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Scheduling\n",
    "Selecting the Scheduler<br>\n",
    "Multiple ways exists to set the scheduler policy for a process\n",
    "- By default, a process will inherit the scheduler from its parent\n",
    "- **chrt** - discussed earlier - can be used to manually select a scheduler\n",
    "- The process can sched_setscheduler to run in a specific scheduler\n",
    "- **systemd** can set the policy and priority when starting the service\n",
    "- - Use **CPUSchedulingPolicy** to select scheduler from a Systemd Unit file\n",
    "- - - Notice that this does not work if **CPUShares** are also set\n",
    "- - User **CPUSchedulingPriority** to set priority from Systemd\n",
    "- See **man systemd.exec** for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
