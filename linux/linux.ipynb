{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proc directory\n",
    "/proc: directory in charge of linux performance optimization\n",
    "\n",
    "proc fidrectory has 3 parts\n",
    "1. PID directory\n",
    "2. information files /meminfo: prevents information from kernel\n",
    "3. /sys: tunables\n",
    "\n",
    "## sys\n",
    "important interface for sys\n",
    "- fs: filesystem\n",
    "- kernel: for kernel\n",
    "- net: networking\n",
    "- vm: memory\n",
    "\n",
    "swappiness: the williness of the kernel to swap\n",
    "out memory pages, if memory pressure arises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systemctl service\n",
    "```systemctl status systemd-sysct```\n",
    "its configuration is in /etc/sysctl.conf<br>\n",
    "changes to system is applied on boot<br>\n",
    "you can change swappiness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/etc/systctl.conf\n",
    "\n",
    "vm.swappiness = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful sysctl command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sysctl --help\n",
    "sysctl -p /etc/sysctl.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# list sysctl prameters\n",
    "sysctl -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Kernel Module Parameters\n",
    "- Use **modinfo** to find which parameters are available\n",
    "- Use **modprobe module key=value** to specify a parameter in runtime\n",
    "- Use /etc/modprobe.d/modulename.conf to specify a permanent parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting Resource Usage\n",
    "\n",
    "### ulimit\n",
    "ulimit is the old way of configuring resource usage<br>\n",
    "Applying POSIX Resource Limits\n",
    "- Set runtime limits with **ulimit**\n",
    "- - It dapplies to the shell in which the command is used\n",
    "- Apply persistent ulimit settings to /etc/security/limits.conf\n",
    "- Soft limits can be modified by the user, hard limits implements an absolute ceiling\n",
    "- - Users can set soft limits, but only lower than the enforced hard limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /etc/security/limits.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Persistent limits\n",
    "- **pam_limits** is applied from PAM sessions to configure persistent limits\n",
    "- It works with /etc/security/limits.conf and /etc/security/limits.d/*.conf\n",
    "- See *man 5 limits.conf** for documentation\n",
    "- - Notice that not all llimites currently work on RHEL. The **rss**limit for instance doesn not work\n",
    "- Example: set the maximum amount of logins for studentst to 2, creating a file with the contents /etc/security/limits/students.d and the following contents:\n",
    "- - **@students hard maxlogins 2**\n",
    "\n",
    "#### Setting Limits to Services\n",
    "- In systemd units, add the **LIMIT\\** entries to the **[Service]** block of a unit file to limit what services can do\n",
    "- If, for inatance you want to allow your blah.service a maximum of 60 seconds of CPU time before it is killed, add the following to **/etc/systemd/system/blah.service.d/10-cpulimits.conf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Service]\n",
    "LimitCPU=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **systemctl daemon-reload** and **systemctl restart blah** to make the changes effective\n",
    "- See **man5 systemd.exec** and **man 2 setrlimit** for a full list of Limit\\* settings\n",
    "\n",
    "PAM (Pluggable Authentication Modules): it shapes what is happening when user log into the system\n",
    "\n",
    "<img src='screenshots/PAM.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "man systemd.exec | grep limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /etc/systemd/system/sleep.service.d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/sleep.service.d/10-cpulimits.conf\n",
    "[Service]\n",
    "LimitCPU=60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# to reload\n",
    "systemctl daemon-reload\n",
    "systemctl restart\n",
    "status sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Groups\n",
    "Understand Control Groups\n",
    "- Control Groups place resources in controllers that represent the type of resource\n",
    "- Some common default controllers are **cpu**, **memory**, and **blkio**\n",
    "- These controllers are subdivided in to a tree structure where different weights or limits are applied to each branch\n",
    "- - Each of these branches is a cgroup\n",
    "- - One or more processes are assigned to a cgroup\n",
    "- Cgroups can be applied from the command line, or from systemd\n",
    "- - Manual creation happened throught the **cgconfig** service and the **cgred** process\n",
    "- - In all cases, cgroup settings are written to /sys/fs/cgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /sys/fs/cgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Allocation \n",
    "- Machine\n",
    "- System\n",
    "- User\n",
    "\n",
    "<img src='screenshots/CGroup.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Cgroups in Systemd\n",
    "Systemd divides **cpu**, **cpuacct**, **blkio** into slices\n",
    "- **system** for system processes and daemons\n",
    "- **machine** for virtual machines\n",
    "- **user** for user sessions\n",
    "On a systemd-system, system-enabled cgroups can be omitted and you can still use **cgconfig** and **cgred**. See **systemd-system.conf** for instructions on how to do that and make sure to rebuild initramfs to make this effective\n",
    "\n",
    "#### Using Custom Slices\n",
    "- Administrators can create their own slices, naming them *.slice, or slices within a slice, using the **parent-child.slice** naming\n",
    "- Child slices will inherit the settings of the parent slices\n",
    "- Make sure to turn on CPU, memory, or I/O accounting to see how they are used within a slice\n",
    "\n",
    "#### Enabling Accounting\n",
    "Enabling accounting in the [Service] section of the unit file\n",
    "- **CPUAccounting=true**\n",
    "- **MemoryAccounting=true**\n",
    "- BlockIOAccounting=true**\n",
    "Or better: enable it in /etc/systemd/system.conf\n",
    "Use drop-in files to take care of this\n",
    "- e.g. the SSH service would use a drop-in /etc/systemd/system/sshd.service.d/*conf<br>\n",
    "**man 5 systemd.resource-control** for all parameters\n",
    "- CPUShares=512\n",
    "- MemoryLimit=512M\n",
    "- BlockIO*="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# listing folder for cpu\n",
    "ls /sys/devices/system/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```top``` then 1 to check cpu usage\n",
    "\n",
    "### Managing slice\n",
    "Putting Commands into a Slice<br>\n",
    "To put a command into a slice, you can use the **systemd-run** command with the **slice=** option\n",
    "- **systemd-run --slice=example.slice sleep 10d**\n",
    "- show with **systemd-cgls /example.slice/<servicename>**\n",
    "- If the **--slice** option is not used, commands started with **systemd-run** will be put into the **system.slice**  \n",
    "    \n",
    "Using Custom Slices\n",
    "- Put a service into a custom slice using **Slice=my.slice**; if the slice doesn't exist it iwll be created when the service is started\n",
    "- You can also pre-create custom slices by creating a *.slice file in /etc/systemd/system. Put the tunables in the **[Slice]** section\n",
    "- To make a lice a child of another slice, git it a name **<parent>-<clild>.slice**; it will inherit all settings of the parent slice\n",
    "- Note that the slice will only be created once the first process is started within\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! systemd-cgtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! systemd-cgls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "- Benchmarking is comparing performance characteristics to industry standards\n",
    "- This means that data needs to be gathered and compared\n",
    "- In IT there are often is no such thing as a standard benchmark, so you'll need to gather and compare a lot \n",
    "- Benchmarking is NOT profiling, which is about gathering information about performance hot spots\n",
    "\n",
    "## Subsystem involved\n",
    "While benchmarking, different subsystems should be involved\n",
    "- Processor\n",
    "- Memory\n",
    "- Scheduler\n",
    "- I/O\n",
    "- Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Utilities\n",
    "- **vmstat**\n",
    "- - Notive that the first line of vmstat output is giving the average since boot!\n",
    "- **iostat**\n",
    "- **mpstat**\n",
    "- **sar**: can be used for gathering performance data\n",
    "- **awk**: is useful for data analysis\n",
    "- **gnuplot**: can be useful for plotting data\n",
    "- **pcp**: is useful as an extensive testing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Performance data with sar\n",
    "**sar**\n",
    "- For filtering purposes, use **LANG=C sar -b**\n",
    "- Create an alias to do this automatically\n",
    "- Consider /etc/sysconfig/sysstat for additional settings\n",
    "- Data collection interval is set through cron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gnuplot\n",
    "you can plot performance data with Gnuplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Information\n",
    "with dmesg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dmsg | less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU, OS and cache Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "lscpu\n",
    "x86info -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Architecture\n",
    "- Cache is organized in lines, and each line can be used to cache a specific location in memory\n",
    "- Each CPU has its separate cache and its own cache controller\n",
    "- If a processor references main memory, it first checks cache for data. If it's there, then that is referred to as a cache hit\n",
    "- A cache line fill occurs after a cache miss, and means that data is loaded from main memory\n",
    "### Write-through and Write-back\n",
    "- If write-through caching is enabled, when a line of cache memory is updated, the line is updated in memory as well\n",
    "- If write-back is enabled, the write to cache is only written to main memory at the moment the cache line is deallocated\n",
    "- Write-back is more efficient, write-through ensures a higher state of stability\n",
    "- If on multi-CPU systems changes are not committed to memory immediately, the other CPUs need to be updated that something is changed if they are caching it also\n",
    "- - This is referred to as cache snooping, which is a hardware feature\n",
    "### Other Cache Features\n",
    "- Direct mapped cache means that each line of cache can only cache a specific location in main memory\n",
    "- - This is the cheaper solution\n",
    "- Fully associative cache means that a cache line can cache any location in main RAM\n",
    "- - More flexible, more expensive\n",
    "- Set associative cache offers a compromise between direct mapped and fully associated cache, nd allows a memory location to be cache into n lines of cache, where n can be a number like 2 for instance\n",
    "### Locality of Reference\n",
    "- Cache memory is most efficient when majority of memory access come from cache\n",
    "- Programs that access memory sequentially benefit most from cache\n",
    "- Somethimes within a program, routines arfe used to make memory access less efficient regarding the way cache is accessed. If that is the case, different **gcc** compiler options can be usede for optimization of cache access\n",
    "- - These options are -O, -O2, -O3, and -Os, and can be passed while compiling using the -f option to gcc\n",
    "- - The -O option are optimizing gcc output for size in different ways. Consult the man page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "System and Library Calls\n",
    "- The kernel exposes system calls to provide kernel access to applications\n",
    "- - While executing system calls, the application claims kernel time, which is known as system time (and visible in top such as)\n",
    "- A library call is the application way of providing functions\n",
    "- - Tiume spent handling library calls is seen as user time in \n",
    "\n",
    "## Using **strace**\n",
    "- **strace**: is used to trace system calls\n",
    "- **strace \\< command \\>**: will show what the command is doing\n",
    "- **strace -p \\< PID \\>**: gives information about a PID\n",
    "- **strace -c \\< command \\>**: shows counters and thus insight on who is doing what\n",
    "- **strace -f**: follows childl processes as sell, which by default is not the case\n",
    "- **strace -e** allows you to follow specific system calls only, as in **strace -e open ls**\n",
    "\n",
    "**ltrace** is similar to strace\n",
    "\n",
    "you can use strace for program that never finishes for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% time     seconds  usecs/call     calls    errors syscall\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      " 22.71    0.000094           9        11           close\n",
      " 21.50    0.000089          45         2           getdents\n",
      " 16.67    0.000069           8         9           openat\n",
      " 12.08    0.000050           5        10           fstat\n",
      "  8.45    0.000035          35         1           write\n",
      "  6.76    0.000028          14         2         2 ioctl\n",
      "  6.28    0.000026           2        17           mmap\n",
      "  5.56    0.000023           3         8         8 access\n",
      "  0.00    0.000000           0         7           read\n",
      "  0.00    0.000000           0        12           mprotect\n",
      "  0.00    0.000000           0         1           munmap\n",
      "  0.00    0.000000           0         3           brk\n",
      "  0.00    0.000000           0         2           rt_sigaction\n",
      "  0.00    0.000000           0         1           rt_sigprocmask\n",
      "  0.00    0.000000           0         1           execve\n",
      "  0.00    0.000000           0         2         2 statfs\n",
      "  0.00    0.000000           0         1           arch_prctl\n",
      "  0.00    0.000000           0         1           set_tid_address\n",
      "  0.00    0.000000           0         1           set_robust_list\n",
      "  0.00    0.000000           0         1           prlimit64\n",
      "------ ----------- ----------- --------- --------- ----------------\n",
      "100.00    0.000414                    93        12 total\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -c ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+++ exited with 0 +++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -e open ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux.ipynb\n",
      "screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\n",
      "access(\"/etc/selinux/config\", F_OK)     = -1 ENOENT (No such file or directory)\n",
      "+++ exited with 0 +++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "strace -e access ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel tracing with **ftrace**\n",
    "ftrace was originally developed for function training in the linux kernel<br>\n",
    "Also gives access to events using static kernel traces\n",
    "- System calls\n",
    "- Scheduler events\n",
    "- Memory management\n",
    "- Interrupts\n",
    "It uses static traces that are present in the kernel by default<br>\n",
    "Using ftrace\n",
    "- Plugin provide new trace types\n",
    "- Exposed via **debugfs** in /sys/kernel/debug/tracing\n",
    "- User-space **trace-cmd** tools are used to access traces\n",
    "- - Results are written yo a file trace.dat\n",
    "- Notice that tracing does cause (a log) of overhead!\n",
    "- - Use filters: **trace-cmd record -e sched_switch -f ' prev)prio < 100'**\n",
    "- - Exclude specific: **trace-cmd record -e sched -v -e \"\\*static\\*'**\n",
    "\n",
    "Using **trace-cmd**\n",
    "- **trace-cmd** uses events and plugins\n",
    "- man pages are available\n",
    "- - **trace-cmd list**: shows available plugins\n",
    "- First, start capturing traces\n",
    "- - **trace-cmd record** dumps all trace data\n",
    "- - **trace0cmd record -p function_traph touch /tmp/file** traces\n",
    "- Use **trace-cmd report** to see the result\n",
    "- - User filtering for more specific results in **trace-cmd report | grep selinux**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "yum install trace-cmd\n",
    "trace-cmd record -p function_graph touch /tmp/file\n",
    "trace-cmd report\n",
    "trace-cmd report | grep selinux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SystemTap\n",
    "SystemTap is for monitory your linux system while running applications for more information read<br>\n",
    "https://sourceware.org/systemtap/SystemTap_Beginners_Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Workflow\n",
    "<img src='screenshots/IO_Workflow.png' style='height: 50%; width: 50%'>\n",
    "## I/O to storage\n",
    "Working with Block Devices\n",
    "- Data is written in blocks\n",
    "- Block devices are accessible through device nodes in /dev\n",
    "- These support seeking for specifc locations\n",
    "- Page cache is sued to optimize performance\n",
    "- - Bypass page cache by using unbuffered I/O\n",
    "- Smart block devices don't need OS level optimization\n",
    "- - They have large caches\n",
    "- - User DMA for memory access\n",
    "- - And do their own I/O scheduling\n",
    "- - With these, you're better off passing I/O directly to the device\n",
    "Understanding I/O Challenges\n",
    "- HDDs have a delay because the read/write head needs to move to the right position\n",
    "- - Seek time is where the hard drive positions the head over the right track\n",
    "- - Rotational delay is where the HDD waits for the right sector to pass under the heads\n",
    "- If data is spread out over the disk, a lot of time is lost\n",
    "- - Disk controller movements can be minimized by re-arranging disk requests\n",
    "- - RHEL does this automatically, putting the requests in a queue and running an elevator algorithm\n",
    "- - In elevator algorithms, starvation can occur in bad algorithms: only floors in the middle are getting serviced\n",
    "\n",
    "I/O Schedulers\n",
    "- **noop**: FIFO: First requests that come in the handled first. Used for SAN, hypervisors and SSD\n",
    "- **deadline**: queued requests for executed in batches defined in the **fifo_batch** parameter. Deadline is good for file and database servers\n",
    "- - Higher values = enhanced throughput\n",
    "- - Lower values = low latencies\n",
    "- - **Read/write expire** define maximum waiting time\n",
    "- - Read requests get priority; give read request more priority by increasing the value in **writes_starved**\n",
    "- **cfq**(complete fair queueing) is useful if many processes are operating on the disk at the same time. Use **ionice** to the cgroup **blkio** controllers to set priorities in this scheduler\n",
    "- - Do NOT user on servers\n",
    "- - Use **ionice -c n -p PID** where n is between 0-7 and 0 is highest priority\n",
    "\n",
    "Selecting I/O Schedulers\n",
    "- As a boot time kernel argument\n",
    "- - elevator=\n",
    "- Throught/sys/block/sda/queue/scheduler\n",
    "- - Each schedudler has its own tuables in /sys/block/sda/queue/iosched\n",
    "- - Change by echoing a new value in the scheduler file\n",
    "- Using tuned profiles\n",
    "- - In [disk] section, use elevator="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management\n",
    "- Memory is prgainized in pages, 4 KiB by default\n",
    "- Processes have a major and minor page fault counter\n",
    "- - *ps* will show you\n",
    "- Systemd can be used to enforce memory limits\n",
    "- Linux uses virtual memory that is mapped to physical memory\n",
    "- - Use **pmap** to display\n",
    "\n",
    "tlb maps virtual memory to resident memory\n",
    "\n",
    "<img src='screenshots/virtual-resident-memory.png' style='height: 50%; width: 50%'>\n",
    "\n",
    "Understanding Memory and Paging\n",
    "- Physical RAM is divided into page frames and the OS uses memory pages tp address memory\n",
    "- - A page size normally is 4KiB\n",
    "- Processes have a virtual address space. From there, physical memory pages are mapped\n",
    "- - Processes can only see their physical memory pages\n",
    "- - On 32 bit systems, the virtual address space is maxed to 4 GiB, on 64 bits address space is 16 EiB\n",
    "- - Notice that the 16 EiB is a theoretical limit\n",
    "- Virtual vs physical memory mappings are monitored using **top** or **ps**\n",
    "- Physical memory pages can be shared between processes. If this is the case, they count to the resident memory of each process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the TLB (Translation Lookaside Buffer)\n",
    "- Each process needs its own page table that contains mapping of virtual addresses to physical addresses\n",
    "- Each virtual page has an entry in this table. If this entry is not mapped to an physical page, this is a page fault\n",
    "- - A Major page fault occurs if a page is swapped out or needs to be loaded from a file on disk\n",
    "- - A minor page fault occurs if the process has just loaded and the virtual page still needs to be mapped to a physical page\n",
    "- - Monitor through /proc/\\< PID \\> /stat\n",
    "- If each process has a complete table, it would require lots of memory\n",
    "- To mitigate that problem, the page table is organized hierarchically, and only page tables that contain physical addresses are administered\n",
    "- Looking up a page is expensive\n",
    "- For that reason, the Translation Look aside Buffer (TLB) is used\n",
    "- - TLB is a hardware cache on CPU\n",
    "- - Depending on hardware, the TLB cache can be organized in L1 and L2, differentiating between instructions and data\n",
    "- - It caches page mappings the process has recently used\n",
    "- Use **x86info -c** to find the size of the TLB\n",
    "- - The TLB is relative small\n",
    "- - To optimize its use, huge pages can be used\n",
    "\n",
    "Memory CoW\n",
    "- New processes are created by forking the parent processes\n",
    "- - A duplicate process is created\n",
    "- - Pages are marked as copy-on-write, which means that data is modified while duplication occurs\n",
    "- Where data can be shared between parent and child, the child has pointers to read-only parts of parent memory address space\n",
    "- The data is only really copied to the child when written to, which leads to performance improvement\n",
    "\n",
    "Managing Proces Memory\n",
    "- Use **ps o pid, comm, minflt, majflt** to get information about page faults that have occurred\n",
    "- - Minor page faults cannot be avoided, major page faults should be avoided\n",
    "- To limit the amount of available memory to process, use Systemd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cgroups\n",
    "[Service]\n",
    "MemoryLimit=4G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- monitor current virtual address space usage using **pmap \\< pid \\>** or **cat/proc/ \\< PID \\> /maps** and **/proc/ \\< PID \\> /smaps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding **pmap** output\n",
    "- **pmap** shows virtual address use - no information about RSS\n",
    "- Notice that not all virutal memory is mapped\n",
    "- - The mapping always startts at 0x00400000 (4 MiB) for the executable part of the process that comes from the executable file\n",
    "- - Next, there is the heap which is memory that has been dynamically allocated using **malloc** and shared libraries\n",
    "- - From address 0x07fffffffffff (128 TiB) on, there is the stack which contains anonymous memory that is requested by the process\n",
    "- - The kernel is available from address -xffff8000000\n",
    "- If a process tries to access memory that doesn't occure in the virtual memory map, the kernel gives a SIGSEGV - a segmentation faul\n",
    "- - If that happens, the program stops and starts core dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pmap `ps | head -2 | cut -d\" \" -f1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uderstanding Memory Leaks\n",
    "- After using it, processes should return memory they have been using, If this doesn't happen, then this is known as a memory leak\n",
    "- Memory leaks are irrelevant for short-time prunning processes such as **ls**, as all memory will be freed by the kernel when it stops running\n",
    "- For daemon processes, meory leaks are a severe problem\n",
    "- The only fix for memory leak is to kill and restart the process\n",
    "\n",
    "### Types of Memory Leak\n",
    "2 types virtual and resident\n",
    "- If a program requests memory but doesn't use it, the virtual size of program memory goes up but no physical memory is used - this is a leark in virtual memory\n",
    "- - The total amount of virtual memory that is allocated is visiable as **Committed_AS** in **/proc/meminfo**\n",
    "- If the program starts mapping resident memory, a problem occurs and you'll suffer from it\n",
    "\n",
    "### Finding Memory leaks\n",
    "- The best tool to find memory leak is **valgrind**. Use **valgrind --tool=memcheck program** to do so\n",
    "- Use **--leak-check=full** as an option to get information about which funciton is leaking memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "valgrind --tool=memcheck ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Reclamation\n",
    "A memory page can be in differetn states\n",
    "- Free: immediately available\n",
    "- Inactive clean: it is not used and its contents is synchronized with corresponding data on disk. It can be treated as a free page, but\n",
    "- - this will result in a page fault in the process accesses it again\n",
    "- - the page needs to be swapped if it is anonymous memory\n",
    "- Inactive dirty: it isn't used but the page contents has not been synchronized to disk\n",
    "- Active: it's doing something\n",
    "Monitoring Memory States\n",
    "- /proc/meminfo gives a generic overview\n",
    "- /proc/PID/smaps shows sizes of Shared/Private clean and dirty memory\n",
    "- Dirty pages need to be written to disk\n",
    "- - Recent Red Hat uses a per-backing device flush thread to flush dirty data, it shows as flush-MAJOR:MINOR\n",
    "\n",
    "MAJOR:MINOR is showned using lsblk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lsblk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing OOM\n",
    "- OOM is out of memory\n",
    "- As most applications never use their entire address space, Linux uses memory overcommitting\n",
    "- As a result, you may get in an OOM situation\n",
    "- Set 3 different modes for overcommiting through **vm.overcommit_memory**\n",
    "- - 0: Heuristic overcommit (default). Overcommitting is allowed, unless it's a very large unrealistic request\n",
    "- - 1: Always overcommit\n",
    "- - 2: Ratio based overcommit: based on available RAM + swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding OOM\n",
    "- It occurs when a minor page fault happens but no free pages are available\n",
    "- If OOM happens, the OOM killer becomes active and will kill one or more processes to free memory\n",
    "- - You can trigger this using **echo f > /proc/sysrq-trigger** and read output in **dmesg**\n",
    "- - Find which trigger by using **echo h > /proc/sysrq-trigger**\n",
    "- This is bad, very bad, which is why alternatively you can set **vm.panic_on_oom = 1** to have the kernel panic instead\n",
    "- Better avoid getting into OOM at all times!\n",
    "\n",
    "## OOM Killer\n",
    "- Every process has an **oom_score** in **/proc/PID/oom_score**\n",
    "- Higher scores are more likely to get killed\n",
    "- The kernel and systemd are immune, root processes, processes with a higher runtime and processes involved in direct hardware ac*. It's value will be added to **oom_score**\n",
    "\n",
    "## Memory Zones\n",
    "- You may get in an OOM situation, evnen if **free** stills reports available memory (in particular on 32-bit systems)\n",
    "- This is because the Linux kernel works in memory zones\n",
    "- - Zone DMA goes from 0 MiB to 16 MiB\n",
    "- - Zone DMA32 goes from 16 MB to 4 GB\n",
    "- - Zone Normal goes from 4 GB to end of available memory\n",
    "- The above is for 64-bit systems. 32-bit systems have low memory up to 896 MiB, all above is high memory\n",
    "- Monitor /proc/boddyinfo to get information aobut these different zones\n",
    "\n",
    "\n",
    "DMA stands for Direct Memory Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysctl -a | grep overcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
