{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Ansible\n",
    "## Automating Device Management\n",
    "- It all started with servers in the server room and administrator managing it\n",
    "- The server room become a datacenter\n",
    "- And after cloud came Internet of Things\n",
    "- How are you going to manage that is a consistent way?\n",
    "\n",
    "## Ansible vs. Shell Scripts\n",
    "- Modern day datacenters need automation\n",
    "- Shell scripting is feasible in small environments only\n",
    "- - Can you install 3 servers as a webserver with a database using shell scripts?\n",
    "- - How easy is it to put different parameters in shell scripts?\n",
    "- - What can you do with a shellscript if and administrator makes an error and removes configuration?\n",
    "- - How easy is it to use a shell script deploying a desired state?\n",
    "- - What can you od with shell scripts across different Linux Distributions?\n",
    "- The answer lies in Configuration Management\n",
    "- In Configuration Management, you'll define a desired state. If current state changes, you'll just re-apply the desired state.\n",
    "\n",
    "## Ansible vs. Puppet and Others\n",
    "- Ansible is just one of the configuration managemnet solutions\n",
    "- Other solutions are Puppet, Chef, Salt, CFEngine and more\n",
    "- Is Ansible really better than the others?\n",
    "- It is definitely easier than many others\n",
    "- And it doesn't need an agent on manged servers, but uses SSH\n",
    "- Ansible is modular which makes it fexible\n",
    "- And the modules by defult written in Python\n",
    "- Ober 1000 modules are already available and administrators who know Pyothon can develop their own Ansible modules\n",
    " \n",
    "## Beyond just Linux\n",
    "- Using Ansible allows you to manage anything\n",
    "- The origins were in managing Linux through SSH\n",
    "- Using different plugins, many other assets can be managed\n",
    "- Different assets can be managed by direct API access\n",
    "- In the end that helps managing the enterprise through Ansible\n",
    "- Modules are written for very differetn assets, to speak the language of the specific managed devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Virtual-Machines\n",
    "- Install VirtualBox, and set up 3 Virtual Machines\n",
    "- - Controller Virtual Machine\n",
    "- - Node1 Virtual Machine\n",
    "- - Node2 Virtual Machine\n",
    "- Follow the instruction to create 3 CEntOS 7.3 Virtual Machine from the \"Virtual Machine\" Tutorial in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up SSH communication between Virtual Machines\n",
    "- First add the Hostnames and IP address of our virtual machines, to the local DNS of our virtual machine by editing the **/etc/hosts** file\n",
    "\n",
    "### Configuring SSH\n",
    "- Set up SSH Key-based authentication \n",
    "- - **ssh-keygen**\n",
    "- This creates a public key as well as a private key\n",
    "- - The server that has the public key sends a challenge that can only be answered with the private key\n",
    "- - Keey the private key in the local user account on the control node\n",
    "- - Send the publkic key to the **~/.ssh/authorized_keys** file in the target user home directory\n",
    "- - - Use **ssh-copy-id user@remotehost**\n",
    "- - - Notice that the lodcal user name and the remote user name do NOT have to be the same\n",
    "- Don't forget to include the controller host as well if you want to manage that also\n",
    "\n",
    "Add the user to wheel, in every VM run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "su -\n",
    "visudo # check if wheel in enabled\n",
    "usermod -aG wheel controller # run in controller vm\n",
    "usermod -aG wheel node1 # run in node1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart all the VMs\n",
    "\n",
    "Now make sure that all the VMs are up\n",
    "- SSH into the controller VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "su -\n",
    "vi /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the configurations in the hosts file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// /etc/hosts\n",
    "127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n",
    "::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n",
    "192.168.56.101   controller.example.com controller\n",
    "192.168.56.102   node1.example.com node1\n",
    "192.168.56.103   node2.example.com node2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copy the hosts file to other vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "scp /etc/hosts node1:/etc/\n",
    "scp /etc/hosts node2:/etc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "su controller\n",
    "ssh-keygen # just use default settings\n",
    "ssh-copy-id node1@node1.example.com\n",
    "ssh-copy-id node2@node2.example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can ssh into node1 and node2 from controller without being prompt for password or passphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "ssh node1@node1\n",
    "ssh node2@node2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Ansible\n",
    "## Prerequisite\n",
    "- You will need a minimum of one Controller node and one Managed host\n",
    "- - All is focused on Linux managed machines\n",
    "- Install Python 2\n",
    "- Add the EPEL (Extra Packages for Enterprise Linux) repository, EPEL repository is managed by the EPEL group.\n",
    "- The EPEL group creates, maintains, and mnages a high quality set of additional packages, that is not included in the core repository. EPEL repository is a part of the non-core repositories of Red Hat distros\n",
    "- Install Ansible from EPEL\n",
    "- Create a non-root user and perform all Ansible tasks as non-root user\n",
    "\n",
    "## Installing Managed Nodes\n",
    "- Same requires as the controller node\n",
    "\n",
    "On all of the VMs run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# on all VMs\n",
    "su -\n",
    "yum install -y python2 epel-release\n",
    "yum install -y ansible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory File\n",
    "Managing Managed Hosts\n",
    "- After installation, you can use the **ansible** command agains remote hsots\n",
    "- Remote hosts need to pe sepcified in the inventory file\n",
    "- The inventory file allows you to defined managed hosts\n",
    "- Hosts are specified by their FQDN (Fully Qualified Domain Name) or IP address\n",
    "- Hosts may be mentioned more than once\n",
    "- - This allows you to create logical groups\n",
    "- - A host may belong to multiple logical groups\n",
    "- In **ansible** commands, you'll mention host names, as well as the inventory file that you're going to use\n",
    "- - **ansible server1.example.com, server2.example.com -i myinventory --list-hosts**\n",
    "\n",
    "## Inventory File Location\n",
    "- The inventory file is indidcated with the **-i** option\n",
    "- Typically, you can create an Ansible project directory in your home directory, and put an inventory file in there\n",
    "\n",
    "## Creating Inventory File\n",
    "- In controller VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "mkdir ~/install\n",
    "cd ~/install\n",
    "touch inventory\n",
    "vi inventory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ~/install/inventory\n",
    "[all]\n",
    "controller@controller.example.com\n",
    "node1@node1.example.com\n",
    "node1@node2.example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inventory file \n",
    "- Identifies all the hosts that ansible can manage\n",
    "- And put all the hosts in a group called \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "ansible all -i inventory --list-hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ansible Configuration File\n",
    "## Managin the Ansible Configuration File\n",
    "- There is no single ansible configuration file, it can be in multiple locations\n",
    "- The ansible.cfg file specifies how Ansible should be used and can be f## Ad-hoc Commandsound in different locations\n",
    "- - The generic file /etc/ansible/ansible.cfg\n",
    "- - The user specific file ~/.ansible.cfg\n",
    "- It's common practice to use ans ansible.cfg file in the project directory\n",
    "- Alternatively, specify \\\\$ANSIBLE_CONFIG environment variable\n",
    "- The ansible.cfg file that is used should contain all environment variables\n",
    "- User **ansible -v** to find out which configuration file is used \n",
    "\n",
    "## ansible.cfg Contents\n",
    "- become: specifies how to escalate privileges to managed hosts, eg whether sudo should be used to escalate priviledges\n",
    "- become_user: specifies which user account to use on the remote host\n",
    "- become_ask_pass: whether or not a password should be asked for\n",
    "- inventory: which inventory file to use\n",
    "- remote_user: name of the user account on the managed machine\n",
    "- - Not set by default, resulting in the local user name being used\n",
    "\n",
    "## Previledge Escalation\n",
    "- Ansible runs tasks on the manageed hosts with the same user account as the local user\n",
    "- - So make sure tha SSH keys are copied to that user's SSH config on remote user\n",
    "- Set **remote_user** in ansible.cfg to specify another user to be used\n",
    "- If **remote_user** is not specified, privilege escalation can be used\n",
    "- Enable in the [privilege_escalation] section in ansible.cfg\n",
    "- - become=True\n",
    "- - become_method=sudo\n",
    "- - become_user=root\n",
    "- - become_ask_pass=False\n",
    "\n",
    "## Copnfiguring sudo for Privilege Escalation\n",
    "- Privilege escalation needs a sudo configuration\n",
    "- For the control node Ansible default account, create a sudo file on all Ansible managed hosts:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# cat /etc/sudoers.d/user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad-hoc Commands\n",
    "Ad-hoc commands are just ansible commands that you run from the command line\n",
    "- You'll typically want to create playbooks to automate tasks against multiple Ansible servers\n",
    "- To quickly make changes to mansy managed hosts, ad-hoc commands are convenient\n",
    "- Ad-hoc commands can also be used for diagnostic purposes, like querying a large number of hosts\n",
    "- In ad-hoc commands, modules are typoically used\n",
    "\n",
    "## Modules\n",
    "- A module is used to accomplish specific tasks in Ansible\n",
    "- - Written in python an run their own specific arguments\n",
    "- Modules can run with their own specific arguments\n",
    "- Modules are specified with the **-m** option, module areguments are referred to with the **-a** option\n",
    "- The default module can be set in ansible.cfg. It's predefined to the **command** module\n",
    "- - This module allows you to run random commands against managed hosts\n",
    "- - As commands is the default module, it doesn't have to be referred to using **-m** module\n",
    "- - Notice that the command omodule is not interpreted by the shell on the managed host and for that reason cannot work with variables, pipes and redirects\n",
    "- - Consider using the **shell** module if you need full shell functionality\n",
    "\n",
    "## 3 Common Modules\n",
    "- **command**: runs a command on a managed host\n",
    "- **shell**: runs a command on managed host through the local shell\n",
    "- **copy**: copy a file, change content on a remote host in a target file\n",
    "\n",
    "## Ad-hoc Command Examples\n",
    "- **ansible all -m command -a id**\n",
    "- - Runs the command module with the **id** command as its argument against all hosts. Notice that this needs [all] to be defined in the inventory\n",
    "- - ansible all refers to all hosts, specified in the inventory\n",
    "- - -m command refers to the default module\n",
    "- - -a id refers to the argument we want to pass to the command module\n",
    "- **ansible all -m command -a id -o**\n",
    "- - Same command, but provide a single line of output\n",
    "- **ansible all -m command -a env**\n",
    "- - Fails, as the command module doesn't work through the shell, it is an internal command\n",
    "- **ansible all -m shell -a env**\n",
    "- **ansible managed1.ansible.local -m copy -a 'content=\"Ansible managed\\n\" dest=/etc/motd'**\n",
    "- - it is a copy command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "ansible all -m command -a id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work because we haven't specified the inventory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "ansible all -i inventory -m command -a id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controller will be unreachable because ssh not install key not setup on controller\n",
    "\n",
    "Run to following command to install ssh key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh controller@controller.example.com\n",
    "ssh-copy-id controller@controller.example.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "ansible all -i inventory -m command -a id\n",
    "ansible all -i inventory -m command -a id -o\n",
    "ansible all -i inventory -m shell -a env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "ansible node1@node1.example.com -i inventory -m copy -a 'content=\"Managed by Ansible\\n\" dest=/etc/motd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will throw this error\n",
    "\n",
    "node1@node1.example.com | FAILED! => {\n",
    "    \"ansible_facts\": {\n",
    "        \"discovered_interpreter_python\": \"/usr/bin/python\"\n",
    "    }, \n",
    "    \"changed\": false, \n",
    "    \"checksum\": \"4458b979ede3c332f8f2128385df4ba305e58c27\", \n",
    "    \"msg\": \"Destination /etc not writable\"\n",
    "}\n",
    "\n",
    "because we need to escalate priviledge and not ass for password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to make an ansible.cfg file that escalate priviledges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "vi ansible.cfg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ~/install/ansible.cfg\n",
    "[privilege_escalation]\n",
    "become=True\n",
    "become_method=sudo\n",
    "become_user=root\n",
    "become_ask_pass=False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// on node1 /etc/sudoers.d/user\n",
    "node1 ALL=(ALL) NOPASSWD: ALL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// on node2 /etc/sudoers.d/user\n",
    "node2 ALL=(ALL) NOPASSWD: ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/install\n",
    "ansible node1@node1.example.com -i inventory -m copy -a 'content=\"Managed by Ansible\\n\" dest=/etc/motd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage by Ansible is copied to /etc/motd file in node1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If We want to Run commands on managed hosts\n",
    "we need to make sure confutration is set up properly\n",
    "- **ansible.cfg** file is set up correctly\n",
    "- **inventory** file is set up correctly\n",
    "- and appropriate **sudo** setup as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Managed Host Summary\n",
    "- Make sure the ansible user account that you're using exists\n",
    "- Copy ssh keys for the ansible user account to the remote host\n",
    "- Log in once, using SSH so that the remote host publick key is stored on the control host\n",
    "- Create sudo configuration\n",
    "- Install pythong and ansibole packages\n",
    "- Update the inventory file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
