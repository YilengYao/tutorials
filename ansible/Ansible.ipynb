{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Ansible\n",
    "## Automating Device Management\n",
    "- It all started with servers in the server room and administrator managing it\n",
    "- The server room become a datacenter\n",
    "- And after cloud came Internet of Things\n",
    "- How are you going to manage that is a consistent way?\n",
    "\n",
    "## Ansible vs. Shell Scripts\n",
    "- Modern day datacenters need automation\n",
    "- Shell scripting is feasible in small environments only\n",
    "- - Can you install 3 servers as a webserver with a database using shell scripts?\n",
    "- - How easy is it to put different parameters in shell scripts?\n",
    "- - What can you do with a shellscript if and administrator makes an error and removes configuration?\n",
    "- - How easy is it to use a shell script deploying a desired state?\n",
    "- - What can you od with shell scripts across different Linux Distributions?\n",
    "- The answer lies in Configuration Management\n",
    "- In Configuration Management, you'll define a desired state. If current state changes, you'll just re-apply the desired state.\n",
    "\n",
    "## Ansible vs. Puppet and Others\n",
    "- Ansible is just one of the configuration managemnet solutions\n",
    "- Other solutions are Puppet, Chef, Salt, CFEngine and more\n",
    "- Is Ansible really better than the others?\n",
    "- It is definitely easier than many others\n",
    "- And it doesn't need an agent on manged servers, but uses SSH\n",
    "- Ansible is modular which makes it fexible\n",
    "- And the modules by defult written in Python\n",
    "- Ober 1000 modules are already available and administrators who know Pyothon can develop their own Ansible modules\n",
    " \n",
    "## Beyond just Linux\n",
    "- Using Ansible allows you to manage anything\n",
    "- The origins were in managing Linux through SSH\n",
    "- Using different plugins, many other assets can be managed\n",
    "- Different assets can be managed by direct API access\n",
    "- In the end that helps managing the enterprise through Ansible\n",
    "- Modules are written for very differetn assets, to speak the language of the specific managed devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Virtual-Machines\n",
    "- Install VirtualBox, and set up 3 Virtual Machines\n",
    "- - Controller Virtual Machine\n",
    "- - Node1 Virtual Machine\n",
    "- - Node2 Virtual Machine\n",
    "- Follow the instruction to create 3 CEntOS 7.3 Virtual Machine from the \"Virtual Machine\" Tutorial in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up SSH communication between Virtual Machines\n",
    "- First add the Hostnames and IP address of our virtual machines, to the local DNS of our virtual machine by editing the **/etc/hosts** file\n",
    "\n",
    "### Configuring SSH\n",
    "- Set up SSH Key-based authentication \n",
    "- - **ssh-keygen**\n",
    "- This creates a public key as well as a private key\n",
    "- - The server that has the public key sends a challenge that can only be answered with the private key\n",
    "- - Keey the private key in the local user account on the control node\n",
    "- - Send the publkic key to the **~/.ssh/authorized_keys** file in the target user home directory\n",
    "- - - Use **ssh-copy-id user@remotehost**\n",
    "- - - Notice that the lodcal user name and the remote user name do NOT have to be the same\n",
    "- Don't forget to include the controller host as well if you want to manage that also\n",
    "\n",
    "Add the user to wheel, in every VM run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "su -\n",
    "visudo # check if wheel in enabled\n",
    "usermod -aG wheel controller # run in controller vm\n",
    "usermod -aG wheel node1 # run in node1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart all the VMs\n",
    "\n",
    "Now make sure that all the VMs are up\n",
    "- SSH into the controller VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "su -\n",
    "vi /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the configurations in the hosts file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// /etc/hosts\n",
    "127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n",
    "::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n",
    "192.168.56.101   controller.example.com controller\n",
    "192.168.56.102   node1.example.com node1\n",
    "192.168.56.103   node2.example.com node2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copy the hosts file to other vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "scp /etc/hosts node1:/etc/\n",
    "scp /etc/hosts node2:/etc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "su controller\n",
    "ssh-keygen # just use default settings\n",
    "ssh-copy-id node1@node1.example.com\n",
    "ssh-copy-id node2@node2.example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can ssh into node1 and node2 from controller without being prompt for password or passphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "ssh node1@node1\n",
    "ssh node2@node2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Ansible\n",
    "## Prerequisite\n",
    "- You will need a minimum of one Controller node and one Managed host\n",
    "- - All is focused on Linux managed machines\n",
    "- Install Python 2\n",
    "- Add the EPEL (Extra Packages for Enterprise Linux) repository, EPEL repository is managed by the EPEL group.\n",
    "- The EPEL group creates, maintains, and mnages a high quality set of additional packages, that is not included in the core repository. EPEL repository is a part of the non-core repositories of Red Hat distros\n",
    "- Install Ansible from EPEL\n",
    "- Create a non-root user and perform all Ansible tasks as non-root user\n",
    "\n",
    "## Installing Managed Nodes\n",
    "- Same requires as the controller node\n",
    "\n",
    "On all of the VMs run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# on all VMs\n",
    "su -\n",
    "yum install -y python2 epel-release\n",
    "yum install -y ansible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory File\n",
    "Managing Managed Hosts\n",
    "- After installation, you can use the **ansible** command agains remote hsots\n",
    "- Remote hosts need to pe sepcified in the inventory file\n",
    "- The inventory file allows you to defined managed hosts\n",
    "- Hosts are specified by their FQDN (Fully Qualified Domain Name) or IP address\n",
    "- Hosts may be mentioned more than once\n",
    "- - This allows you to create logical groups\n",
    "- - A host may belong to multiple logical groups\n",
    "- In **ansible** commands, you'll mention host names, as well as the inventory file that you're going to use\n",
    "- - **ansible server1.example.com, server2.example.com -i myinventory --list-hosts**\n",
    "\n",
    "## Inventory File Location\n",
    "- The inventory file is indidcated with the **-i** option\n",
    "- Typically, you can create an Ansible project directory in your home directory, and put an inventory file in there\n",
    "\n",
    "## Creating Inventory File\n",
    "- In controller VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "mkdir ~/install\n",
    "cd ~/install\n",
    "touch inventory\n",
    "vi inventory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ~/install/inventory\n",
    "[all]\n",
    "controller.example.com\n",
    "node1.example.com\n",
    "node2.example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inventory file \n",
    "- Identifies all the hosts that ansible can manage\n",
    "- And put all the hosts in a group called \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in controller VM\n",
    "ansible all -i inventory --list-hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ansible Configuration File\n",
    "## Managin the Ansible Configuration File\n",
    "- There is no single ansible configuration file, it can be in multiple locations\n",
    "- The ansible.cfg file specifies how Ansible should be used and can be f## Ad-hoc Commandsound in different locations\n",
    "- - The generic file /etc/ansible/ansible.cfg\n",
    "- - The user specific file ~/.ansible.cfg\n",
    "- It's common practice to use ans ansible.cfg file in the project directory\n",
    "- Alternatively, specify \\\\$ANSIBLE_CONFIG environment variable\n",
    "- The ansible.cfg file that is used should contain all environment variables\n",
    "- User **ansible -v** to find out which configuration file is used \n",
    "\n",
    "## ansible.cfg Contents\n",
    "- become: specifies how to escalate privileges to managed hosts, eg whether sudo should be used to escalate priviledges\n",
    "- become_user: specifies which user account to use on the remote host\n",
    "- become_ask_pass: whether or not a password should be asked for\n",
    "- inventory: which inventory file to use\n",
    "- remote_user: name of the user account on the managed machine\n",
    "- - Not set by default, resulting in the local user name being used\n",
    "\n",
    "## Previledge Escalation\n",
    "- Ansible runs tasks on the manageed hosts with the same user account as the local user\n",
    "- - So make sure tha SSH keys are copied to that user's SSH config on remote user\n",
    "- Set **remote_user** in ansible.cfg to specify another user to be used\n",
    "- If **remote_user** is not specified, privilege escalation can be used\n",
    "- Enable in the [privilege_escalation] section in ansible.cfg\n",
    "- - become=True\n",
    "- - become_method=sudo\n",
    "- - become_user=root\n",
    "- - become_ask_pass=False\n",
    "\n",
    "## Copnfiguring sudo for Privilege Escalation\n",
    "- Privilege escalation needs a sudo configuration\n",
    "- For the control node Ansible default account, create a sudo file on all Ansible managed hosts:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# cat /etc/sudoers.d/user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
